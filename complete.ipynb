{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AIR WRITING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "# Function to get the index finger tip and thumb tip positions\n",
    "def get_finger_tips(frame, hand_landmarks):\n",
    "    if hand_landmarks:\n",
    "        index_finger_tip = hand_landmarks.landmark[mp.solutions.hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "        thumb_tip = hand_landmarks.landmark[mp.solutions.hands.HandLandmark.THUMB_TIP]\n",
    "        height, width, _ = frame.shape\n",
    "        index_finger_tip_px = (int(index_finger_tip.x * width), int(index_finger_tip.y * height))\n",
    "        thumb_tip_px = (int(thumb_tip.x * width), int(thumb_tip.y * height))\n",
    "        return index_finger_tip_px, thumb_tip_px\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Function to calculate distance between two points\n",
    "def calculate_distance(point1, point2):\n",
    "    return math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "# Function to perform smoothing\n",
    "def smooth_line(new_point, prev_points, smoothing_factor=0.5):\n",
    "    if prev_points is None:\n",
    "        return new_point\n",
    "    else:\n",
    "        smoothed_point = tuple(np.round(smoothing_factor * np.array(new_point) + (1 - smoothing_factor) * np.array(prev_points)).astype(int))\n",
    "        return smoothed_point\n",
    "\n",
    "def main():\n",
    "    # Open camera\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Initialize MediaPipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(max_num_hands=1)\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    # Create a blank canvas filled with white color\n",
    "    canvas = np.ones((480, 640, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    # Variables for drawing and erasing\n",
    "    drawing = False\n",
    "    prev_point = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert the BGR image to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame with MediaPipe Hands\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        # Draw hand landmarks on the frame\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Get finger tip positions\n",
    "        index_finger_tip, thumb_tip = get_finger_tips(frame, results.multi_hand_landmarks[0] if results.multi_hand_landmarks else None)\n",
    "\n",
    "        if index_finger_tip is not None and thumb_tip is not None:\n",
    "            distance_threshold = 50  # Adjust threshold as needed\n",
    "            distance = calculate_distance(index_finger_tip, thumb_tip)\n",
    "            \n",
    "            if distance < distance_threshold:\n",
    "                drawing = False\n",
    "                # Check if index finger is touching the top part of the frame\n",
    "                if index_finger_tip[1] < 50:  # Adjust the value as needed\n",
    "                    # Implement clear, save, quit functionality\n",
    "                    if index_finger_tip[0] < 213:  # 640/3, 3 sections\n",
    "                        canvas = np.ones((480, 640, 3), dtype=np.uint8) * 255  # Clear canvas\n",
    "                    elif 213 <= index_finger_tip[0] < 426:  # 640/3 * 2\n",
    "                        cv2.imwrite('air_canvas.png', canvas)  # Save canvas as image\n",
    "                    else:\n",
    "                        break  # Quit program\n",
    "\n",
    "            else:\n",
    "                drawing = True\n",
    "\n",
    "        if drawing:\n",
    "            cv2.circle(frame, index_finger_tip, 5, (0, 0, 0), -1)  # Draw in black color\n",
    "            if prev_point is not None:\n",
    "                smoothed_point = smooth_line(index_finger_tip, prev_point)\n",
    "                cv2.line(canvas, prev_point, smoothed_point, (0, 0, 0), 4)  # Draw in black color\n",
    "                prev_point = smoothed_point\n",
    "            else:\n",
    "                prev_point = index_finger_tip\n",
    "        else:\n",
    "            prev_point = None\n",
    "        \n",
    "        # Display canvas\n",
    "        cv2.imshow('Canvas', canvas)\n",
    "\n",
    "        # Overlay drawing onto the frame\n",
    "        frame_with_drawing = cv2.bitwise_and(frame, canvas)\n",
    "        \n",
    "        cv2.putText(frame_with_drawing, \"Clear\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "        cv2.putText(frame_with_drawing, \"Save\", (220, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        cv2.putText(frame_with_drawing, \"Quit\", (420, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # Display frame\n",
    "        cv2.imshow('Frame', frame_with_drawing)\n",
    "\n",
    "        # Check for key press\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'hello world.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "image_path = \"C:/Users/aswin/OneDrive/Desktop/Work/IBM/air_canvas.png\"\n",
    "# Create the image-to-text pipeline using the TrOCR model\n",
    "pipe = pipeline(\"image-to-text\", model=\"microsoft/trocr-base-handwritten\")\n",
    "prediction = pipe(image_path)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "c:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\donut\\feature_extraction_donut.py:28: FutureWarning: The class DonutFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DonutImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<s_cord-v2><s_menu><s_nm> Aawir</s_nm><s_price> Aawir</s_nm><s_price>'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "image_path = \"air_canvas.png\"\n",
    "pipe = pipeline(\"image-to-text\", model=\"jinhybr/OCR-Donut-CORD\")\n",
    "prediction = pipe(image_path)\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
